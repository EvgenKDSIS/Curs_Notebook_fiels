{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, SpatialDropout1D, Flatten, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPRINT_WIDTH = 160 # константа для функции pprint, количество символов в одной строке при выводе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/kaggle/input/train-nlp/train.json'\n",
    "\n",
    "with open(path_train, 'r') as f:\n",
    "    data_train = json.loads(f.read())\n",
    "\n",
    "text_data = pd.DataFrame(data_train)\n",
    "text_data.drop(columns = 'id', inplace = True)\n",
    "print(text_data.shape)\n",
    "print(text_data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = text_data['sentiment'].nunique()\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  сделаем разделение наших данных на обучение тест с учетом стратификации\n",
    "train_index, test_index = train_test_split(np.arange(text_data.shape[0]), stratify = text_data['sentiment'])\n",
    "\n",
    "x_train_raw = text_data.iloc[train_index, 0].values\n",
    "y_train_raw = text_data.iloc[train_index, 1].values\n",
    "x_test_raw = text_data.iloc[test_index, 0].values\n",
    "y_test_raw = text_data.iloc[test_index, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 20000 # количество слов в словаре\n",
    "\n",
    "# объявляем наш tokenizer\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,\n",
    "                      filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n",
    "                      lower=True, split=' ', char_level=False, oov_token='UNKNOWN'\n",
    "                     )\n",
    "# обучаем tokenizer на текстах, составляем словарь частотности\n",
    "tokenizer.fit_on_texts(x_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим наши тексты в последовательность индексов (токенов) с помощью tokenizer\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train_raw)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объявим функцию для чистки наших последовательностей от тега unknown\n",
    "# мы предполагаем, что наличие тега unknown не несет значимой информации\n",
    "def drop_UNKNOWN (x_seq, unknown=1):\n",
    "    x_seq_short = []\n",
    "    for x in x_seq:\n",
    "        x_ = np.array(x)\n",
    "        x_ = x_[np.where(x_ !=unknown )]\n",
    "        x_seq_short.append(list(x_))\n",
    "    return x_seq_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# очистим наши последовательности, полученные из обучающей и тестовой выборок\n",
    "# от тега unknown с использованием объявленной функции\n",
    "x_train_seq_short = drop_UNKNOWN(x_train_seq)\n",
    "x_test_seq_short = drop_UNKNOWN(x_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# устанавливаем максимальную длинну последовательности токенов\n",
    "MAX_LEN_SEQ = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вырвниваем длинну всех последовательностей токенов до MAX_LEN_SEQ\n",
    "# при помощи стандартного инструмента pad_sequence, входящего в Keras\n",
    "# при этом последовательности короче MAX_LEN_SEQ будут дополнены нулями\n",
    "# а последовательности длиннее MAX_LEN_SEQ будут обрезаны\n",
    "\n",
    "x_train_emb = pad_sequences(x_train_seq_short, padding='post', maxlen=MAX_LEN_SEQ)\n",
    "x_test_emb = pad_sequences(x_test_seq_short, padding='post', maxlen=MAX_LEN_SEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объявляем кодировщик для целевого признака (класса отзыва)\n",
    "# используем стандартный OneHotEncoder из библиотеки Sklearn\n",
    "target_encoeder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# обучаем наш кодировщик на целевых призаках обучающей выборки\n",
    "target_encoeder.fit(y_train_raw.reshape([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объявляем переменную classes_names, в которой сохраним матрицу\n",
    "# с названиями классов (тип отзыва)\n",
    "# названия классов понадобятся нам на этапе инференса\n",
    "classes_names = target_encoeder.categories_[0]\n",
    "classes_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переведем целевые переменные для обучающей и тестовой выборки в формат OHE\n",
    "# это нужно для подачи в модель\n",
    "\n",
    "y_train_01 = target_encoeder.transform(y_train_raw.reshape([-1, 1]))\n",
    "y_test_01 = target_encoeder.transform(y_test_raw.reshape([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEmb = Sequential() # объявляем нашу модель как последовательность слоев\n",
    "# добавляем слой Embedding\n",
    "modelEmb.add(Embedding(input_dim=NUM_WORDS, output_dim=200, input_length=MAX_LEN_SEQ))\n",
    "# добавляем слой SpatialDropout1D для \"прореживания\" и борьбы с переобучением\n",
    "modelEmb.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# Добавляем слой долго-краткосрочной памяти (400 элементов для долговременного хранения информации, отключаем входной сигнал с вероятностью 20%, отключаем рекуррентный сигнал с вероятностью 20%)\n",
    "modelEmb.add(LSTM(400, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "\n",
    "# добавим выравнивающий слой\n",
    "modelEmb.add(Flatten())\n",
    "# добавим Dense слой на 16 нейронов\n",
    "modelEmb.add(Dense(96,  activation='relu'))\n",
    "# добавим Dense слой на 16 нейронов\n",
    "modelEmb.add(Dense(16,  activation='relu'))\n",
    "# добавим выходной полносвязный слой для классификации\n",
    "modelEmb.add(Dense(n_classes,activation='softmax'))\n",
    "modelEmb.add(Dropout(0.2))\n",
    "# компилируем модель\n",
    "modelEmb.compile(optimizer=Adam(learning_rate=0.001),  loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "# выводим данные по модели\n",
    "modelEmb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем модель\n",
    "modelEmb.fit(x = x_train_emb,  y = y_train_01, epochs = 10, verbose = 1, \n",
    "             validation_data= (x_test_emb, y_test_01))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
