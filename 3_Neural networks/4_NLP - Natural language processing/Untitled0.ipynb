{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGw5VCH1qaM9"
   },
   "source": [
    "# Задание\n",
    "Выполните следующие задания:\n",
    "\n",
    "> 1. Лёгкий уровень (Задание не обязательно к выполнению)\n",
    "Подставьте в нейронные сети из п.3.4.3 (шпаргалки) по распознаванию тональностей новостей слой LSTM и обучите нейронные сети. Сделайте выводы, улучшила ли подстановка слоя качество распознавания тональностей.\n",
    "\n",
    "\n",
    "> 2. PRO уровень (Задание не обязательно к выполнению)\n",
    "Найдите код модели классификации BERT. Возьмите русскую модель, доучите её и запустите классификацию тональностей отзывов на данной модели. Можно не добиваться высоких показателей метрик — примеров для обучения мало для существенного роста метрик модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV-R-OlIH7sq"
   },
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMr89jKKH89g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, SpatialDropout1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMum2Z-aIcEM"
   },
   "outputs": [],
   "source": [
    "PPRINT_WIDTH = 160 # константа для функции pprint, количество символов в одной строке при выводе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rVChqtkl-kt"
   },
   "outputs": [],
   "source": [
    "def acc_loss(model):\n",
    "    acc = model.history['accuracy']\n",
    "    val_acc = model.history['val_accuracy']\n",
    "\n",
    "    loss = model.history['loss']\n",
    "    val_loss = model.history['val_loss']\n",
    "\n",
    "    # построение графика точности\n",
    "    plt.plot(acc, 'r', label='Train Accuracy')\n",
    "    plt.plot(val_acc, 'b', label='Validation Accuracy')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # построение графика ошибки\n",
    "    plt.plot(loss, 'r', label='Train Loss')\n",
    "    plt.plot(val_loss, 'b', label='Validation Loss')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "LzqqLKYMIe1y",
    "outputId": "ea6aac1f-c3d8-4b5f-9e5e-74e820090101"
   },
   "outputs": [],
   "source": [
    "path_train = './train.json'\n",
    "\n",
    "text_data = pd.read_json(path_train)\n",
    "text_data.drop(columns = 'id', inplace = True)\n",
    "print(text_data.shape)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84WWNC-mIhWx",
    "outputId": "bfce03a6-f20e-48d0-a101-06ebb99726ce"
   },
   "outputs": [],
   "source": [
    "text_data.shape # выведем размерность нашего DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GEdVMGNIowr"
   },
   "outputs": [],
   "source": [
    "#  сделаем разделение наших данных на обучение тест с учетом стратификации\n",
    "train_index, test_index = train_test_split(np.arange(text_data.shape[0]), stratify = text_data['sentiment'])\n",
    "\n",
    "x_train_raw = text_data.iloc[train_index, 0].values\n",
    "y_train_raw = text_data.iloc[train_index, 1].values\n",
    "x_test_raw = text_data.iloc[test_index, 0].values\n",
    "y_test_raw = text_data.iloc[test_index, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVZ06d8PIp8c"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = 20000 # константа, максимальное количество слов, которые будет учитывать наша модель\n",
    "\n",
    "# создадим наш Токенайзер\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, # максимальное количество слов, которые будет учитывать наш токенайзер\n",
    "                      filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff', # символы, которые он токенайзер будет фильтровать\n",
    "                      lower=True,  #приводятся ли все символы к нижнему регситру\n",
    "                      split=' ',  # символ, по которому происходит разделение на слова (токены)\n",
    "                      char_level=False,  # являются ли токенами отдельные буквы\n",
    "                      oov_token='UNKNOWN' # токен для неизвестных слов\n",
    "                     )\n",
    "\n",
    "tokenizer.fit_on_texts(x_train_raw) # обучим наш токенайзер на обучающих текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epq49uNfIzRH"
   },
   "outputs": [],
   "source": [
    "x_train_seq = tokenizer.texts_to_sequences(x_train_raw) # преобразуем наши тексты в последовательность токенов (индексов слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6E4qgM4PIzxH",
    "outputId": "aa293cbd-8fcc-41ca-be65-69fd7b1c2e40"
   },
   "outputs": [],
   "source": [
    "# а теперь попробуем сделать обратное декодирование нашей последовательности в текст\n",
    "# это позволит нам понять, какие данные видит нейронная сеть\n",
    "text = ' '\n",
    "for i in x_train_seq[1]:\n",
    "    text += tokenizer.index_word[i] + ' '\n",
    "pprint(text, width=PPRINT_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3sueaJyIzzm"
   },
   "outputs": [],
   "source": [
    "# Преобразуем наши текстовые данные в формат One Hot Encoding\n",
    "x_train_01 = tokenizer.texts_to_matrix(x_train_raw)\n",
    "x_test_01 = tokenizer.texts_to_matrix(x_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "N1tXe5-jIz2G",
    "outputId": "d8eda410-59b4-453f-84dd-2c52524f3f33"
   },
   "outputs": [],
   "source": [
    "# закодируем нашу целевую переменную (класс отзыва) в формат OHE для подачи в нейронную сеть\n",
    "target_encoeder = OneHotEncoder(sparse=False) # создадим объект Encoder\n",
    "target_encoeder.fit(y_train_raw.reshape([-1, 1])) # обучим его на целевом признаке из обучающих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-byyqgVsIz5F",
    "outputId": "25bcad51-037b-47f8-9cc3-7f9777a6e5d1"
   },
   "outputs": [],
   "source": [
    "# сохраним названия классов в отдельную переменную, это понадобится нам на этапе предсказани\n",
    "classes_names = target_encoeder.categories_[0] # названия классов хранятся в .categories_[0]\n",
    "classes_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_KHigoLIz8E"
   },
   "outputs": [],
   "source": [
    "# приведем наши целевые переменные из называний в формат OHO для подачи в нейронную сеть\n",
    "y_train_01 = target_encoeder.transform(y_train_raw.reshape([-1, 1]))\n",
    "y_test_01 = target_encoeder.transform(y_test_raw.reshape([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiVKJ1A1OupC"
   },
   "outputs": [],
   "source": [
    "# подсчитаем количество уникальных классов отзывов\n",
    "n_classes = text_data['sentiment'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVRNoYYpTBoO"
   },
   "outputs": [],
   "source": [
    "# переводим наши тексты в последовательность индексов (токенов) с помощью tokenizer\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train_raw)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvlcWR0yS-Ph"
   },
   "outputs": [],
   "source": [
    "# объявим функцию для чистки наших последовательностей от тега unknown\n",
    "# мы предполагаем, что наличие тега unknown не несет значимой информации\n",
    "def drop_UNKNOWN (x_seq, unknown=1):\n",
    "    x_seq_short = []\n",
    "    for x in x_seq:\n",
    "        x_ = np.array(x)\n",
    "        x_ = x_[np.where(x_ !=unknown )]\n",
    "        x_seq_short.append(list(x_))\n",
    "    return x_seq_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW-xLE7YSzxv"
   },
   "outputs": [],
   "source": [
    "# устанавливаем максимальную длинну последовательности токенов\n",
    "MAX_LEN_SEQ = 2000\n",
    "\n",
    "# очистим наши последовательности, полученные из обучающей и тестовой выборок\n",
    "# от тега unknown с использованием объявленной функции\n",
    "x_train_seq_short = drop_UNKNOWN(x_train_seq)\n",
    "x_test_seq_short = drop_UNKNOWN(x_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7avk0UsSx4C"
   },
   "outputs": [],
   "source": [
    "# вырвниваем длинну всех последовательностей токенов до MAX_LEN_SEQ\n",
    "# при помощи стандартного инструмента pad_sequence, входящего в Keras\n",
    "# при этом последовательности короче MAX_LEN_SEQ будут дополнены нулями\n",
    "# а последовательности длиннее MAX_LEN_SEQ будут обрезаны\n",
    "\n",
    "x_train_emb = pad_sequences(x_train_seq_short, padding='post', maxlen=MAX_LEN_SEQ)\n",
    "x_test_emb = pad_sequences(x_test_seq_short, padding='post', maxlen=MAX_LEN_SEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXn_Do6XJEZ1",
    "outputId": "4b7f4b43-daf1-4658-853d-e2ef1b7cc00c"
   },
   "outputs": [],
   "source": [
    "# объявим нашу модель\n",
    "modelEmb = Sequential() # объявляем нашу модель как последовательность слоев\n",
    "# добавляем слой Embedding\n",
    "modelEmb.add(Embedding(input_dim=NUM_WORDS, output_dim=200, input_length=MAX_LEN_SEQ))\n",
    "# добавляем слой SpatialDropout1D для \"прореживания\" и борьбы с переобучением\n",
    "modelEmb.add(SpatialDropout1D(0.5))\n",
    "# добавим выравнивающий слой\n",
    "modelEmb.add(Flatten())\n",
    "# добавим Dense слой на 16 нейронов\n",
    "modelEmb.add(Dense(16,  activation='relu'))\n",
    "# добавим батч-нормализацию для борьбы с переобучением\n",
    "modelEmb.add(BatchNormalization())\n",
    "# добавим прореживание для борьбы с переобучением\n",
    "modelEmb.add(Dropout(0.2))\n",
    "# добавим выходной полносвязный слой для классификации\n",
    "modelEmb.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# компилируем модель\n",
    "modelEmb.compile(optimizer=Adam(learning_rate=0.001),  loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "# выводим данные по модели\n",
    "modelEmb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_mj2OeBKw8y",
    "outputId": "9da3dab3-e849-4888-b703-d88649bcdff2"
   },
   "outputs": [],
   "source": [
    "model = modelEmb.fit(x = x_train_emb,  y = y_train_01, epochs = 10, verbose = 1, validation_data= (x_test_emb, y_test_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "ceb2D691JEcQ",
    "outputId": "b90aa117-c1af-4970-fc8d-1acbdd87553f"
   },
   "outputs": [],
   "source": [
    "acc_loss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJtjKDMPJEe-"
   },
   "outputs": [],
   "source": [
    "# объявим нашу модель LSTM\n",
    "# Создание модели\n",
    "model_LSTM = Sequential() # объявляем нашу модель как последовательность слоев\n",
    "# добавляем слой Embedding\n",
    "model_LSTM.add(Embedding(input_dim=NUM_WORDS, output_dim=200, input_length=MAX_LEN_SEQ))\n",
    "\n",
    "# добавляем слой SpatialDropout1D для \"прореживания\" и борьбы с переобучением\n",
    "model_LSTM.add(SpatialDropout1D(0.5))\n",
    "\n",
    "# Добавляем слой долго-краткосрочной памяти (400 элементов для долговременного хранения информации, отключаем входной сигнал с вероятностью 20%, отключаем рекуррентный сигнал с вероятностью 20%)\n",
    "model_LSTM.add(LSTM(400, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# добавим выравнивающий слой\n",
    "model_LSTM.add(Flatten())\n",
    "\n",
    "# добавим Dense слой на 96 нейронов\n",
    "model_LSTM.add(Dense(96, activation='relu'))\n",
    "\n",
    "# добавим Dense слой на 16 нейронов\n",
    "model_LSTM.add(Dense(16, activation='relu'))\n",
    "\n",
    "# добавим выходной полносвязный слой для классификации\n",
    "model_LSTM.add(Dense(n_classes,activation='softmax'))\n",
    "model_LSTM.add(Dropout(0.2))\n",
    "\n",
    "# компилируем модель\n",
    "model_LSTM.compile(optimizer=Adam(learning_rate=0.001),  loss='categorical_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "QAkr4v6AJEhm",
    "outputId": "464fa099-67e3-4f42-a401-4d9681b3b31e"
   },
   "outputs": [],
   "source": [
    "model = model_LSTM.fit(x=x_train_emb, y=y_train_01, epochs=10, verbose=1, validation_data=(x_test_emb, y_test_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzTY7fJMP5TR"
   },
   "outputs": [],
   "source": [
    "acc_loss(model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
